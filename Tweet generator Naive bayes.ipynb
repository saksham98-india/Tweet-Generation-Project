{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c3f5de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# library required\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2cd06a2",
   "metadata": {},
   "source": [
    "Using ML alogrithum to generate tweet as naive bayes classifer is used for text classification. It predicts the class or category of a given text based on the features extracted from the training data.\n",
    "\n",
    "Let's load our dataset.\n",
    "\n",
    "The Dataset have some encoding. So, I have applied encoding parameter so our kernel can read the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f441d277",
   "metadata": {},
   "outputs": [],
   "source": [
    "header = ['Target','Ids','Date','Flag','User','Tweet']\n",
    "df = pd.read_csv(r\"H:\\My Drive\\Intershala internship\\Kudosware\\training.1600000.processed.noemoticon.csv\",encoding='ISO-8859–1', names=header)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920fd98b",
   "metadata": {},
   "source": [
    "So now, To clean the tweets and to take out useful parameter I used nltk library and some function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1ccbe4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['Text'] = df['Tweet'].str.lower()\n",
    "df['Text'] = df['Text'].map(lambda s: ' '.join([x for x in s.split() if 'http' not in x if '@' not in x]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5665a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saksh\\AppData\\Local\\Temp\\ipykernel_14408\\94996927.py:1: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['processed_tweets'] = df['Text'].str.replace('[^\\w\\s]', '').str.lower()\n"
     ]
    }
   ],
   "source": [
    "df['processed_tweets'] = df['Text'].str.replace('[^\\w\\s]', '').str.lower()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f0b5f9",
   "metadata": {},
   "source": [
    "Let's separate each work by using word_tokenizer and also remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4df89e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokenized_tweets'] = df['processed_tweets'].apply(word_tokenize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c083f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "df['filtered_tweets'] = df['tokenized_tweets'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "# df['filtered_tweets'] = df['filtered_tweets'].map(lambda y: ' '.join([word for word in y]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8e94d7",
   "metadata": {},
   "source": [
    "As we are training our model on the text so we remove stopwords the model will not learn the right way to write a tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d32f38d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target</th>\n",
       "      <th>Ids</th>\n",
       "      <th>Date</th>\n",
       "      <th>Flag</th>\n",
       "      <th>User</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Text</th>\n",
       "      <th>processed_tweets</th>\n",
       "      <th>tokenized_tweets</th>\n",
       "      <th>filtered_tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "      <td>- awww, that's a bummer. you shoulda got david...</td>\n",
       "      <td>awww thats a bummer you shoulda got david car...</td>\n",
       "      <td>[awww, thats, a, bummer, you, shoulda, got, da...</td>\n",
       "      <td>[awww, thats, bummer, shoulda, got, david, car...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>is upset that he can't update his facebook by ...</td>\n",
       "      <td>is upset that he cant update his facebook by t...</td>\n",
       "      <td>[is, upset, that, he, cant, update, his, faceb...</td>\n",
       "      <td>[upset, cant, update, facebook, texting, might...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>i dived many times for the ball. managed to sa...</td>\n",
       "      <td>i dived many times for the ball managed to sav...</td>\n",
       "      <td>[i, dived, many, times, for, the, ball, manage...</td>\n",
       "      <td>[dived, many, times, ball, managed, save, 50, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>[my, whole, body, feels, itchy, and, like, its...</td>\n",
       "      <td>[whole, body, feels, itchy, like, fire]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>no, it's not behaving at all. i'm mad. why am ...</td>\n",
       "      <td>no its not behaving at all im mad why am i her...</td>\n",
       "      <td>[no, its, not, behaving, at, all, im, mad, why...</td>\n",
       "      <td>[behaving, im, mad, cant, see]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599995</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601966</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>AmandaMarie1028</td>\n",
       "      <td>Just woke up. Having no school is the best fee...</td>\n",
       "      <td>just woke up. having no school is the best fee...</td>\n",
       "      <td>just woke up having no school is the best feel...</td>\n",
       "      <td>[just, woke, up, having, no, school, is, the, ...</td>\n",
       "      <td>[woke, school, best, feeling, ever]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599996</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601969</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>TheWDBoards</td>\n",
       "      <td>TheWDB.com - Very cool to hear old Walt interv...</td>\n",
       "      <td>thewdb.com - very cool to hear old walt interv...</td>\n",
       "      <td>thewdbcom  very cool to hear old walt intervie...</td>\n",
       "      <td>[thewdbcom, very, cool, to, hear, old, walt, i...</td>\n",
       "      <td>[thewdbcom, cool, hear, old, walt, interviews, â]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599997</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601991</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>bpbabe</td>\n",
       "      <td>Are you ready for your MoJo Makeover? Ask me f...</td>\n",
       "      <td>are you ready for your mojo makeover? ask me f...</td>\n",
       "      <td>are you ready for your mojo makeover ask me fo...</td>\n",
       "      <td>[are, you, ready, for, your, mojo, makeover, a...</td>\n",
       "      <td>[ready, mojo, makeover, ask, details]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599998</th>\n",
       "      <td>4</td>\n",
       "      <td>2193602064</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>tinydiamondz</td>\n",
       "      <td>Happy 38th Birthday to my boo of alll time!!! ...</td>\n",
       "      <td>happy 38th birthday to my boo of alll time!!! ...</td>\n",
       "      <td>happy 38th birthday to my boo of alll time tup...</td>\n",
       "      <td>[happy, 38th, birthday, to, my, boo, of, alll,...</td>\n",
       "      <td>[happy, 38th, birthday, boo, alll, time, tupac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599999</th>\n",
       "      <td>4</td>\n",
       "      <td>2193602129</td>\n",
       "      <td>Tue Jun 16 08:40:50 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>RyanTrevMorris</td>\n",
       "      <td>happy #charitytuesday @theNSPCC @SparksCharity...</td>\n",
       "      <td>happy #charitytuesday</td>\n",
       "      <td>happy charitytuesday</td>\n",
       "      <td>[happy, charitytuesday]</td>\n",
       "      <td>[happy, charitytuesday]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1600000 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Target         Ids                          Date      Flag  \\\n",
       "0             0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1             0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2             0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3             0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4             0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "...         ...         ...                           ...       ...   \n",
       "1599995       4  2193601966  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599996       4  2193601969  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599997       4  2193601991  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599998       4  2193602064  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599999       4  2193602129  Tue Jun 16 08:40:50 PDT 2009  NO_QUERY   \n",
       "\n",
       "                    User                                              Tweet  \\\n",
       "0        _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...   \n",
       "1          scotthamilton  is upset that he can't update his Facebook by ...   \n",
       "2               mattycus  @Kenichan I dived many times for the ball. Man...   \n",
       "3                ElleCTF    my whole body feels itchy and like its on fire    \n",
       "4                 Karoli  @nationwideclass no, it's not behaving at all....   \n",
       "...                  ...                                                ...   \n",
       "1599995  AmandaMarie1028  Just woke up. Having no school is the best fee...   \n",
       "1599996      TheWDBoards  TheWDB.com - Very cool to hear old Walt interv...   \n",
       "1599997           bpbabe  Are you ready for your MoJo Makeover? Ask me f...   \n",
       "1599998     tinydiamondz  Happy 38th Birthday to my boo of alll time!!! ...   \n",
       "1599999   RyanTrevMorris  happy #charitytuesday @theNSPCC @SparksCharity...   \n",
       "\n",
       "                                                      Text  \\\n",
       "0        - awww, that's a bummer. you shoulda got david...   \n",
       "1        is upset that he can't update his facebook by ...   \n",
       "2        i dived many times for the ball. managed to sa...   \n",
       "3           my whole body feels itchy and like its on fire   \n",
       "4        no, it's not behaving at all. i'm mad. why am ...   \n",
       "...                                                    ...   \n",
       "1599995  just woke up. having no school is the best fee...   \n",
       "1599996  thewdb.com - very cool to hear old walt interv...   \n",
       "1599997  are you ready for your mojo makeover? ask me f...   \n",
       "1599998  happy 38th birthday to my boo of alll time!!! ...   \n",
       "1599999                              happy #charitytuesday   \n",
       "\n",
       "                                          processed_tweets  \\\n",
       "0         awww thats a bummer you shoulda got david car...   \n",
       "1        is upset that he cant update his facebook by t...   \n",
       "2        i dived many times for the ball managed to sav...   \n",
       "3           my whole body feels itchy and like its on fire   \n",
       "4        no its not behaving at all im mad why am i her...   \n",
       "...                                                    ...   \n",
       "1599995  just woke up having no school is the best feel...   \n",
       "1599996  thewdbcom  very cool to hear old walt intervie...   \n",
       "1599997  are you ready for your mojo makeover ask me fo...   \n",
       "1599998  happy 38th birthday to my boo of alll time tup...   \n",
       "1599999                               happy charitytuesday   \n",
       "\n",
       "                                          tokenized_tweets  \\\n",
       "0        [awww, thats, a, bummer, you, shoulda, got, da...   \n",
       "1        [is, upset, that, he, cant, update, his, faceb...   \n",
       "2        [i, dived, many, times, for, the, ball, manage...   \n",
       "3        [my, whole, body, feels, itchy, and, like, its...   \n",
       "4        [no, its, not, behaving, at, all, im, mad, why...   \n",
       "...                                                    ...   \n",
       "1599995  [just, woke, up, having, no, school, is, the, ...   \n",
       "1599996  [thewdbcom, very, cool, to, hear, old, walt, i...   \n",
       "1599997  [are, you, ready, for, your, mojo, makeover, a...   \n",
       "1599998  [happy, 38th, birthday, to, my, boo, of, alll,...   \n",
       "1599999                            [happy, charitytuesday]   \n",
       "\n",
       "                                           filtered_tweets  \n",
       "0        [awww, thats, bummer, shoulda, got, david, car...  \n",
       "1        [upset, cant, update, facebook, texting, might...  \n",
       "2        [dived, many, times, ball, managed, save, 50, ...  \n",
       "3                  [whole, body, feels, itchy, like, fire]  \n",
       "4                           [behaving, im, mad, cant, see]  \n",
       "...                                                    ...  \n",
       "1599995                [woke, school, best, feeling, ever]  \n",
       "1599996  [thewdbcom, cool, hear, old, walt, interviews, â]  \n",
       "1599997              [ready, mojo, makeover, ask, details]  \n",
       "1599998  [happy, 38th, birthday, boo, alll, time, tupac...  \n",
       "1599999                            [happy, charitytuesday]  \n",
       "\n",
       "[1600000 rows x 10 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ff5d0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = 0\n",
    "for token in df['processed_tweets']:\n",
    "    vocab += len(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c81b2f",
   "metadata": {},
   "source": [
    "So to convert a collection of raw text documents into a numerical format i have used TfidfVectorizer as it also have basic pre-processing of nltk which will be help for our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5665af69",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector= TfidfVectorizer(max_features=vocab)\n",
    "train= vector.fit_transform(df['processed_tweets'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f49e73",
   "metadata": {},
   "source": [
    "Applied my MultinomialNB model to our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb4e230f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MNB = MultinomialNB()\n",
    "MNB.fit(train,df['Target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fa5a67",
   "metadata": {},
   "source": [
    "Ignore the above text.\n",
    "\n",
    "Created a function to generator text with the help of our model as it generates the next word based on the predicted class by randomly choosing a word from the corresponding subset in the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7139896",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_generator(seed_sentence):\n",
    "    \n",
    "    vectorized_sentence = vector.transform([seed_sentence])\n",
    "    prediction = MNB.predict(vectorized_sentence)[0]\n",
    "    if prediction == 0:\n",
    "        next_word = np.random.choice(df[df['Target']==0]['processed_tweets'])\n",
    "    elif prediction == 2:\n",
    "        next_word = np.random.choice(df[df['Target']==2]['processed_tweets'])\n",
    "    else:\n",
    "        next_word = np.random.choice(df[(df['Target']==4)]['processed_tweets'])\n",
    "    seed_sentence += \" \" + next_word\n",
    "    \n",
    "    return seed_sentence\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be73ed5",
   "metadata": {},
   "source": [
    "As I mentioned above that the model randomly choose word so the tweet generatered will hve no sense and we can post it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1905462f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have getting ready for work its the last week with my kids and seeing that cuty\n"
     ]
    }
   ],
   "source": [
    "seed_sentence = \"I have\"\n",
    "generated_text = text_generator(seed_sentence)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b95c8fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I love good band\n"
     ]
    }
   ],
   "source": [
    "seed_sentence = \"I love\"\n",
    "generated_text = text_generator(seed_sentence)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8da9cba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "World is finally got myself back in the gym today and feeling much more like myself againthank you gb\n"
     ]
    }
   ],
   "source": [
    "seed_sentence = \"World is\"\n",
    "generated_text = text_generator(seed_sentence)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b71fa538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My machine learning drinking a beer and relaxing after riding across kansas on her bike and rehearsing for the show all day\n"
     ]
    }
   ],
   "source": [
    "seed_sentence = \"My machine learning\"\n",
    "generated_text = text_generator(seed_sentence)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "07ff2b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am missing kk already\n"
     ]
    }
   ],
   "source": [
    "seed_sentence = \"I am\"\n",
    "generated_text = text_generator(seed_sentence)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c6360834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "awesome day that airport is my daughters namesake\n"
     ]
    }
   ],
   "source": [
    "seed_sentence = \"awesome day\"\n",
    "generated_text = text_generator(seed_sentence)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13fa98b",
   "metadata": {},
   "source": [
    "So I have tried a new approach which is LSTM but requires a lot of processing which a TPU can provide.\n",
    "\n",
    "I have tried implementing it but as i don't have access to TPU so i was not able to tune it or go further."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
